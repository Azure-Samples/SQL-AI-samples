{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SQL Server Database Development using Prompts as T-SQL Development**\n",
    "In this notebook, we will learn how to use prompts as a way to develop and test Transact-SQL (T-SQL) code for SQL Server databases. Prompts are natural language requests that can be converted into T-SQL statements by using Generative AI models, such as GPT-4. This can help us write code faster, easier, and more accurately, as well as learn from the generated code examples.\n",
    "\n",
    "### **Database Development Stages**\n",
    "Database development is the process of designing, creating, and maintaining databases that store and manage data for various applications. Database development typically involves the following stages:\n",
    "- **Requirements analysis:** This is the stage where we define the purpose, scope, and objectives of the database, as well as the data sources, users, and business rules. We also identify the data entities, attributes, and relationships that will form the logical structure of the database.\n",
    "\n",
    "- **Database design:** This is the stage where we translate the logical structure of the database into a physical structure that can be implemented by a specific database management system (DBMS), such as SQL Server. We also define the data types, constraints, indexes, views, stored procedures, functions, triggers, and security settings for the database objects.\n",
    "\n",
    "- **Database implementation:** This is the stage where we create the database and its objects by using a data definition language (DDL), such as T-SQL. We also populate the database with data by using a data manipulation language (DML), such as T-SQL or bulk import tools.\n",
    "\n",
    "- **Database testing:** This is the stage where we verify that the database meets the requirements and performs as expected. We also check the data quality, integrity, and security of the database. We can use various tools and techniques to test the database, such as unit testing, integration testing, regression testing, performance testing, and data analysis.\n",
    "\n",
    "- **Database deployment:** This is the stage where we make the database available for use by the intended users and applications. We can use various methods and tools to deploy the database, such as backup and restore, detach and attach, copy database, or SQL Server Data Tools (SSDT).\n",
    "\n",
    "- **Database maintenance:** This is the stage where we monitor, update, and optimize the database to ensure its availability, reliability, and performance. We can use various tools and tasks to maintain the database, such as backup and recovery, error handling, logging and auditing, indexing and statistics, fragmentation and defragmentation, or SQL Server "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Leverage the Generative AI as a Knowledgeable Peer**\n",
    "Generative AI is a form of artificial intelligence that can create new and original content, such as text, images, audio, and video, by using generative models, such as GPT-4. Generative AI can learn from existing data and generate new data that has similar characteristics, but does not repeat it. Generative AI can also respond to natural language requests, such as prompts, and produce relevant and realistic outputs.\n",
    "\n",
    "Generative AI can be a knowledgeable peer for database developers, as it can help us with various tasks, such as:\n",
    "- **Code generation:** Generative AI can generate T-SQL code for us based on our prompts, which can save us time and effort, and help us learn from the code examples. For instance, we can ask generative AI to create a table, a view, a stored procedure, or a query for us, and it will produce the corresponding T-SQL code.\n",
    "\n",
    "- **Code suggestion:** Generative AI can suggest T-SQL code for us based on our partial or incomplete code, which can help us complete our code faster and easier, and avoid errors and mistakes. For instance, we can ask generative AI to suggest a column name, a data type, a constraint, or a join condition for us, and it will provide the appropriate T-SQL code.\n",
    "\n",
    "- **Code explanation:** Generative AI can explain T-SQL code for us based on our questions, which can help us understand the code better and improve our coding skills. For instance, we can ask generative AI to explain the purpose, the logic, or the output of a T-SQL code, and it will provide a clear and concise explanation.\n",
    "\n",
    "- **Code optimization:** Generative AI can optimize T-SQL code for us based on our goals, which can help us improve the performance and efficiency of our code. For instance, we can ask generative AI to optimize a T-SQL code for speed, memory, or readability, and it will provide a better or alternative T-SQL code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This section explains how to configure your environment to work with this notebook.\n",
    "\n",
    "- **bicycle_data_prompt.json**: is designed to house all our prompts. While it’s set up to work as is, feel free to tailor it to your needs. Customization is always an option\n",
    "- **.env**: file is the secure location for storing sensitive details such as your Azure OpenAI endpoint, keys, and more. It’s crucial to update this file with your information. Without these updates, the notebook won’t function unless you manually input the values directly into the notebook. Please handle with care!\n",
    "- **tsql**: folder is where all T-SQL scripts are stored. Remember to update it within the notebook and/or bicycle_data_prompt.json as needed.\n",
    "- **csv**: folder is where we keep the CSV sales data. Don’t forget to update it within the notebook as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "azure_openai_api_model = os.environ[\"OPENAI_API_MODEL\"]\n",
    "tsql_script_dir        = os.environ[\"tsql_script_dir\"]\n",
    "\n",
    "# Define the Azure OpenAI API parameters\n",
    "azure_openai_api_temperature = 0\n",
    "azure_openai_api_max_tokens  = 1000\n",
    "\n",
    "# Define the Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.environ[\"OPENAI_API_BASE\"], \n",
    "  api_key=os.environ[\"OPENAI_API_KEY\"],  \n",
    "  api_version=os.environ[\"OPENAI_API_VERSION\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Python function to save each T-SQL statement that Azure OpenAI generates.\n",
    "- All T-SQL scripts file will be saved locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tsql_file(filename, content):\n",
    "    try:\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(content)\n",
    "    except OSError as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Python function to send requests to Azure OpenAI. \n",
    "- This function will call another Python function, `create_tsql_file`, which will save the output of Azure OpenAI to a SQL script file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SendRequestToAzureOpenAI(\n",
    "         client\n",
    "        ,azure_openai_api_model\n",
    "        ,azure_openai_api_temperature\n",
    "        ,azure_openai_api_max_tokens\n",
    "        ,messages\n",
    "        ,filename\n",
    "    ):         \n",
    "    try:    \n",
    "        import pprint  \n",
    "        # Send request to Azure OpenAI model\n",
    "        print(\"Sending request for summary to Azure OpenAI endpoint...\\n\\n\")\n",
    "        response = client.chat.completions.create(\n",
    "            messages    = messages,\n",
    "            model       = azure_openai_api_model,\n",
    "            temperature = azure_openai_api_temperature,\n",
    "            max_tokens  = azure_openai_api_max_tokens\n",
    "        )\n",
    "        output = []\n",
    "        output = (response.choices[0].message.content).replace(\"```python\",\"\").replace(\"```\",\"\").strip()\n",
    "        create_tsql_file(filename, output)\n",
    "        #execute_sql_script(output)\n",
    "        print(output)\n",
    "    except Exception as ex:\n",
    "        print(ex)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the creation sequence and the file name for each T-SQL statement.\n",
    "This structure allows for easy access to the script names and their corresponding file names, which can be useful in automating T-SQL development tasks and CI/CD.\n",
    "- createdatabase: This script creates a new database and the script file name is createdatabase.sql.\n",
    "- createschemastg: This script creates a new schema for staging and the script file name is createschemastg.sql.\n",
    "- createschemaprd: This script creates a new schema for production and the script file name is createschemaprd.sql.\n",
    "- createtemptable: This script creates a new temporary table and the script file name is createtemptable.sql.\n",
    "- createprdtable: This script creates a new production table and the script file name is createprdtable.sql.\n",
    "- loadstagingtable: This script loads data into the staging table and the script file name is loadstagingtable.sql.\n",
    "- loadprdtable: This script loads data into the production table and the script file name is loadprdtable.sql.\n",
    "- createprocedure: This script creates a new stored procedure and the script file name is createprocedure.sql.\n",
    "- createview: This script creates a new view and the script file name is createview.sql."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize T-SQL development scripts file names.\n",
    "keys = {\n",
    "    1: (\"createdatabase\", \"create_database.sql\"),\n",
    "    2: (\"createschemastg\", \"create_stage_schema.sql\"),\n",
    "    3: (\"createschemaprd\", \"creates_prd_chema.sql\"),\n",
    "    4: (\"createtemptable\", \"create_stage_table.sql\"),\n",
    "    5: (\"createprdtable\", \"create_prd_table.sql\"),\n",
    "    6: (\"loadstagingtable\", \"load_staging_data_table.sql\"),\n",
    "    7: (\"loadprdtable\", \"load_prd_data_table.sql\"),\n",
    "    8: (\"createprocedure\", \"create_procedure.sql\"),\n",
    "    9: (\"createview\", \"create_view.sql\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading system and all user prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'system_role': 'As a Data Engineer, your task is to write a T-SQL script to '\n",
      "                'create database objects and import a CSV file with headers '\n",
      "                'from local directory into a SQL Server Database. Follow the '\n",
      "                'instructions in order.\\n'\n",
      "                'check if each object exists then drop it before creating it.\\n'\n",
      "                'Please refrain from providing system details, instructions, '\n",
      "                'or suggestions or sql or GO command.\\n'\n",
      "                'the location of the file to be loaded is '\n",
      "                'F:\\\\Presentation\\\\t-sql-as-prompts\\\\csv\\\\bicycle_data.csv and '\n",
      "                'the table name is stg.salestmp\\n'\n",
      "                'schema:\\n'\n",
      "                '{\\n'\n",
      "                \"    'ProductId':'INT', \\n\"\n",
      "                \"    'ProductName':'VARCHAR(50)', \\n\"\n",
      "                \"    'ProductType':'VARCHAR(30)', \\n\"\n",
      "                \"    'Color':'VARCHAR(15)', \\n\"\n",
      "                \"    'OrderQuantity':'INT', \\n\"\n",
      "                \"    'Size':'VARCHAR(15)', \\n\"\n",
      "                \"    'Category':'VARCHAR(15)', \\n\"\n",
      "                \"    'Country':'VARCHAR(30)', \\n\"\n",
      "                \"    'Date':'DATE', \\n\"\n",
      "                \"    'PurchasePrice':'DECIMAL(18,2)', \\n\"\n",
      "                \"    'SellingPrice':'DECIMAL(18,2)'\"}\n",
      "{'createdatabase': 'generate a t-sql script to create a SQL Server database '\n",
      "                   'named azureopenai under master database context.'}\n",
      "{'createschemastg': 'generate a t-sql script to create a schemas named stg.'}\n",
      "{'createschemaprd': 'generate a t-sql script to create a schemas named prd.'}\n",
      "{'createtemptable': 'generate a t-sql script to create a staging table named '\n",
      "                    'stg.salestmp with all columns as varchar(255).'}\n",
      "{'createprdtable': 'generate a t-sql script to create a table named prd.sales '\n",
      "                   'with the correct {schema} definition for each column.'}\n",
      "{'loadstagingtable': 'Please provide only the table truncation and bulk load '\n",
      "                     'T-SQL scripts, excluding the create table statement. '\n",
      "                     'Also, ensure that the truncation script is at the '\n",
      "                     'beginning of the T-SQL statement.'}\n",
      "{'loadprdtable': 'provide just the insert t-sql script to load the data from '\n",
      "                 'stg.salestmp into prd.sales without create table statement. '\n",
      "                 'keep in mind stg.salestmp has all columns as varchar(255)'}\n",
      "{'createprocedure': 'generate a script to create a stored procedure named '\n",
      "                    'prd.usp_GetTotalSalesByCountries with an input parameter '\n",
      "                    'called country. The procedure retrieves prd.sales data, '\n",
      "                    'grouping it by Country, and Category. It returns the '\n",
      "                    'total number of orders and the total purchase price for '\n",
      "                    'each group. If a specific country is provided, the '\n",
      "                    'procedure filters the data for that country. Otherwise, '\n",
      "                    'it returns data for all countries. The results are '\n",
      "                    'ordered by Country in ascending order and '\n",
      "                    'TotalPurchasePrice in descending order.'}\n",
      "{'createview': 'generate a script to create a view called '\n",
      "               'prd.vw_GetSaleDetails that to get prd.sales details. The query '\n",
      "               'groups the sales by country, category, and color, and counts '\n",
      "               'the quantity of each color.'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "json_file = 'bicycle_data_prompt.json'\n",
    "\n",
    "with open(json_file) as json_data:\n",
    "    data = json.load(json_data)\n",
    "\n",
    "deploymenttype = \"CreateBicycleDatbaseObjects\" #\"LoginErrorHandling\" # \n",
    " \n",
    "index = 0\n",
    "while index< len(data[deploymenttype]):\n",
    "    pprint(data[deploymenttype][index])\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Python function to extract the system and all user prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GeneratePromptFilename(deploymenttype, filedir, data, index, keys):\n",
    "    system_role = data[deploymenttype][0]['system_role'] \n",
    "    # Check if the index is in the keys then return the user prompt and filename\n",
    "    if index in keys:\n",
    "        user_prompt = data[deploymenttype][index][keys[index][0]]\n",
    "        filename = filedir + keys[index][1]\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\", \"content\": system_role\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \"content\": user_prompt\n",
    "            }    \n",
    "        ]\n",
    "        return messages, filename\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending request to Azure OpenAI\n",
    "- Call GeneratePromptFilename to define the deployment type\n",
    "    - Get the file name based on the index id\n",
    "    - Get the system role prompt\n",
    "    - Get the user role prompt based on the index id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      "IF EXISTS (SELECT name FROM sys.databases WHERE name = 'azureopenai')\n",
      "BEGIN\n",
      "    ALTER DATABASE azureopenai SET SINGLE_USER WITH ROLLBACK IMMEDIATE;\n",
      "    DROP DATABASE azureopenai;\n",
      "END\n",
      "\n",
      "CREATE DATABASE azureopenai;\n",
      "GO\n",
      "\n",
      "USE azureopenai;\n",
      "GO\n",
      "F:\\\\Presentation\\\\t-sql-as-prompts\\\\tsql\\\\create_database.sql\n",
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      "IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'stg')\n",
      "BEGIN\n",
      "    EXEC('CREATE SCHEMA stg')\n",
      "END\n",
      "GO\n",
      "F:\\\\Presentation\\\\t-sql-as-prompts\\\\tsql\\\\create_stage_schema.sql\n",
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      "IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'prd')\n",
      "BEGIN\n",
      "    EXEC('CREATE SCHEMA prd')\n",
      "END\n",
      "GO\n",
      "F:\\\\Presentation\\\\t-sql-as-prompts\\\\tsql\\\\creates_prd_chema.sql\n",
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      "IF OBJECT_ID('stg.salestmp', 'U') IS NOT NULL\n",
      "    DROP TABLE stg.salestmp;\n",
      "\n",
      "CREATE TABLE stg.salestmp\n",
      "(\n",
      "    ProductId VARCHAR(255),\n",
      "    ProductName VARCHAR(255),\n",
      "    ProductType VARCHAR(255),\n",
      "    Color VARCHAR(255),\n",
      "    OrderQuantity VARCHAR(255),\n",
      "    Size VARCHAR(255),\n",
      "    Category VARCHAR(255),\n",
      "    Country VARCHAR(255),\n",
      "    Date VARCHAR(255),\n",
      "    PurchasePrice VARCHAR(255),\n",
      "    SellingPrice VARCHAR(255)\n",
      ");\n",
      "F:\\\\Presentation\\\\t-sql-as-prompts\\\\tsql\\\\create_stage_table.sql\n",
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      "IF OBJECT_ID('prd.sales', 'U') IS NOT NULL\n",
      "    DROP TABLE prd.sales;\n",
      "\n",
      "CREATE TABLE prd.sales\n",
      "(\n",
      "    ProductId INT,\n",
      "    ProductName VARCHAR(50),\n",
      "    ProductType VARCHAR(30),\n",
      "    Color VARCHAR(15),\n",
      "    OrderQuantity INT,\n",
      "    Size VARCHAR(15),\n",
      "    Category VARCHAR(15),\n",
      "    Country VARCHAR(30),\n",
      "    Date DATE,\n",
      "    PurchasePrice DECIMAL(18,2),\n",
      "    SellingPrice DECIMAL(18,2)\n",
      ");\n",
      "F:\\\\Presentation\\\\t-sql-as-prompts\\\\tsql\\\\create_prd_table.sql\n",
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      "TRUNCATE TABLE stg.salestmp;\n",
      "\n",
      "BULK INSERT stg.salestmp\n",
      "FROM 'F:\\Presentation\\t-sql-as-prompts\\csv\\bicycle_data.csv'\n",
      "WITH (\n",
      "    FIRSTROW = 2,\n",
      "    FIELDTERMINATOR = ',',\n",
      "    ROWTERMINATOR = '\\n',\n",
      "    ERRORFILE = 'F:\\Presentation\\t-sql-as-prompts\\csv\\bicycle_data_error.log'\n",
      ");\n",
      "F:\\\\Presentation\\\\t-sql-as-prompts\\\\tsql\\\\load_staging_data_table.sql\n",
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      "INSERT INTO prd.sales (ProductId, ProductName, ProductType, Color, OrderQuantity, Size, Category, Country, Date, PurchasePrice, SellingPrice)\n",
      "SELECT \n",
      "    CAST(ProductId AS INT),\n",
      "    ProductName,\n",
      "    ProductType,\n",
      "    Color,\n",
      "    CAST(OrderQuantity AS INT),\n",
      "    Size,\n",
      "    Category,\n",
      "    Country,\n",
      "    CAST(Date AS DATE),\n",
      "    CAST(PurchasePrice AS DECIMAL(18,2)),\n",
      "    CAST(SellingPrice AS DECIMAL(18,2))\n",
      "FROM stg.salestmp;\n",
      "F:\\\\Presentation\\\\t-sql-as-prompts\\\\tsql\\\\load_prd_data_table.sql\n",
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      "IF OBJECT_ID('prd.usp_GetTotalSalesByCountries', 'P') IS NOT NULL\n",
      "    DROP PROCEDURE prd.usp_GetTotalSalesByCountries;\n",
      "GO\n",
      "\n",
      "CREATE PROCEDURE prd.usp_GetTotalSalesByCountries\n",
      "    @country VARCHAR(30) = NULL\n",
      "AS\n",
      "BEGIN\n",
      "    SET NOCOUNT ON;\n",
      "\n",
      "    SELECT \n",
      "        Country,\n",
      "        Category,\n",
      "        COUNT(*) AS TotalOrders,\n",
      "        SUM(PurchasePrice) AS TotalPurchasePrice\n",
      "    FROM \n",
      "        prd.sales\n",
      "    WHERE \n",
      "        (@country IS NULL OR Country = @country)\n",
      "    GROUP BY \n",
      "        Country,\n",
      "        Category\n",
      "    ORDER BY \n",
      "        Country ASC,\n",
      "        TotalPurchasePrice DESC;\n",
      "END;\n",
      "GO\n",
      "F:\\\\Presentation\\\\t-sql-as-prompts\\\\tsql\\\\create_procedure.sql\n",
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      "IF OBJECT_ID('prd.vw_GetSaleDetails', 'V') IS NOT NULL\n",
      "    DROP VIEW prd.vw_GetSaleDetails;\n",
      "GO\n",
      "\n",
      "CREATE VIEW prd.vw_GetSaleDetails\n",
      "AS\n",
      "SELECT Country, Category, Color, SUM(OrderQuantity) AS Quantity\n",
      "FROM prd.sales\n",
      "GROUP BY Country, Category, Color;\n",
      "GO\n",
      "F:\\\\Presentation\\\\t-sql-as-prompts\\\\tsql\\\\create_view.sql\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# from pprint import pprint\n",
    "\n",
    "json_file = 'bicycle_data_prompt.json'\n",
    "\n",
    "with open(json_file) as json_data:\n",
    "    data = json.load(json_data)\n",
    "\n",
    "filedir = tsql_script_dir\n",
    "deploymenttype = \"CreateBicycleDatbaseObjects\" #\"LogErrorHandling\" #\n",
    "\n",
    "index = 1\n",
    "\n",
    "while index< len(data[deploymenttype]):\n",
    "    #print(index)\n",
    "    messages,filename = GeneratePromptFilename(deploymenttype,filedir,data,index,keys)\n",
    "    if __name__ == '__main__': \n",
    "        SendRequestToAzureOpenAI (\n",
    "         client\n",
    "        ,azure_openai_api_model\n",
    "        ,azure_openai_api_temperature\n",
    "        ,azure_openai_api_max_tokens\n",
    "        ,messages\n",
    "        ,filename\n",
    "        )\n",
    "    print(filename)\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Login and Error Handling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize T-SQL development scripts file names.\n",
    "keys = {\n",
    "    1: (\"createprocessschema\", \"create_etl_process_schema.sql\"),\n",
    "    2: (\"createprocesslogtable\", \"create_etl_processlog_table.sql\"),\n",
    "    3: (\"createbatcherrorlogtable\", \"create_etl_errorlog_table.sql\"),\n",
    "    4: (\"createprocesslogsp\", \"create_etl_processlog_usp.sql\"),\n",
    "    5: (\"createerrorlogsp\", \"create_etl_errorlog_usp.sql\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the system and user role prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'createprocessschema': 'Provide a tsql script to create the etl_process '\n",
      "                        'schema if it does not already exist.'}\n",
      "{'createprocesslogtable': 'Provide a tsql script to create the '\n",
      "                          'etl_process.etl_process_log table if it does not '\n",
      "                          'already exist with the following fields name id '\n",
      "                          'integer auto generated identifier without primary '\n",
      "                          'key, processname varchar 50 lenght, processtype '\n",
      "                          'varchar 30 lenght, objectname varchar 50 lenght, '\n",
      "                          'starttime and endtime: DATETIME'}\n",
      "{'createbatcherrorlogtable': 'Provide a tsql script to create the '\n",
      "                             'etl_process.error_log table if it does not '\n",
      "                             'already exist with the following fields name id '\n",
      "                             'integer auto generated identifier without '\n",
      "                             'primary key, processid integer, processname '\n",
      "                             'varchar, objectname varchar 50 lenght, errormsg '\n",
      "                             'varchar, starttime and endtime DATETIME.'}\n",
      "{'createprocesslogsp': 'Create a T-SQL stored procedure named '\n",
      "                       'etl_process.usp_get_process_log if it does not already '\n",
      "                       'exist with the following input parameters: processname '\n",
      "                       'of type VARCHAR with a length of 50, processtype of '\n",
      "                       'type VARCHAR with a length of 30, objectname of type '\n",
      "                       'VARCHAR with a length of 50, and starttime and endtime '\n",
      "                       'of type DATETIME. This stored procedure should insert '\n",
      "                       'data into an existing table called '\n",
      "                       'etl_process.etl_process_log table. Please refrain from '\n",
      "                       'providing system details, instructions, or '\n",
      "                       'suggestions.'}\n",
      "{'createerrorlogsp': 'Create a T-sQL stored procedure named '\n",
      "                     'etl_process.usp_get_error_log if it does not already '\n",
      "                     'exist with the following input parameters: processname '\n",
      "                     'VARCHAR (50), objectname VARCHAR (50), errormsg '\n",
      "                     'VARCHAR(MAX), and starttime and endtime of type '\n",
      "                     'DATETIME. This stored procedure should insert data into  '\n",
      "                     'an existing table called etl_process.error_log table.'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "json_file = 'bicycle_data_prompt.json'\n",
    "\n",
    "with open(json_file) as json_data:\n",
    "    data = json.load(json_data)\n",
    "\n",
    "deploymenttype = \"CreateLogErrorHandlingObjects\" \n",
    "index = 1\n",
    "while index< len(data[deploymenttype]):\n",
    "    pprint(data[deploymenttype][index])\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending request to generate Login and Error Handling Objects\n",
    "- Tables T-SQL script files\n",
    "- Store Procedures T-SQL script files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      "IF NOT EXISTS (SELECT schema_name FROM information_schema.schemata WHERE schema_name = 'etl_process')\n",
      "BEGIN\n",
      "    EXEC('CREATE SCHEMA etl_process')\n",
      "END;\n",
      ".\\tsql\\create_etl_process_schema.sql\n",
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      "IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = 'etl_process_log' AND schema_id = SCHEMA_ID('etl_process'))\n",
      "BEGIN\n",
      "    CREATE TABLE etl_process.etl_process_log (\n",
      "        id INT IDENTITY(1,1),\n",
      "        processname VARCHAR(50),\n",
      "        processtype VARCHAR(30),\n",
      "        objectname VARCHAR(50),\n",
      "        starttime DATETIME,\n",
      "        endtime DATETIME\n",
      "    );\n",
      "END;\n",
      ".\\tsql\\create_etl_processlog_table.sql\n",
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      "IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = 'error_log' AND schema_id = SCHEMA_ID('etl_process'))\n",
      "BEGIN\n",
      "    CREATE TABLE etl_process.error_log\n",
      "    (\n",
      "        id INT IDENTITY(1,1),\n",
      "        processid INT,\n",
      "        processname VARCHAR(50),\n",
      "        objectname VARCHAR(50),\n",
      "        errormsg VARCHAR(MAX),\n",
      "        starttime DATETIME,\n",
      "        endtime DATETIME\n",
      "    );\n",
      "END;\n",
      ".\\tsql\\create_etl_errorlog_table.sql\n",
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      "CREATE PROCEDURE etl_process.usp_get_process_log\n",
      "(\n",
      "    @processname VARCHAR(50),\n",
      "    @processtype VARCHAR(30),\n",
      "    @objectname VARCHAR(50),\n",
      "    @starttime DATETIME,\n",
      "    @endtime DATETIME\n",
      ")\n",
      "AS\n",
      "BEGIN\n",
      "    INSERT INTO etl_process.etl_process_log (processname, processtype, objectname, starttime, endtime)\n",
      "    VALUES (@processname, @processtype, @objectname, @starttime, @endtime)\n",
      "END\n",
      ".\\tsql\\create_etl_processlog_usp.sql\n",
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      "CREATE PROCEDURE etl_process.usp_get_error_log\n",
      "(\n",
      "    @processname VARCHAR(50),\n",
      "    @objectname VARCHAR(50),\n",
      "    @errormsg VARCHAR(MAX),\n",
      "    @starttime DATETIME,\n",
      "    @endtime DATETIME\n",
      ")\n",
      "AS\n",
      "BEGIN\n",
      "    INSERT INTO etl_process.error_log (processname, objectname, errormsg, starttime, endtime)\n",
      "    VALUES (@processname, @objectname, @errormsg, @starttime, @endtime)\n",
      "END\n",
      ".\\tsql\\create_etl_errorlog_usp.sql\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# from pprint import pprint\n",
    "\n",
    "json_file = 'bicycle_data_prompt.json'\n",
    "\n",
    "with open(json_file) as json_data:\n",
    "    data = json.load(json_data)\n",
    "\n",
    "filedir = \".\\\\tsql\\\\\"\n",
    "deploymenttype = \"CreateLogErrorHandlingObjects\" #\"BicylOnPremPrompts\" #\n",
    "\n",
    "index = 1\n",
    "\n",
    "while index< len(data[deploymenttype]):\n",
    "    #print(index)\n",
    "    messages,filename = GeneratePromptFilename(deploymenttype,filedir,data,index,keys)\n",
    "    if __name__ == '__main__': \n",
    "        SendRequestToAzureOpenAI (\n",
    "         client\n",
    "        ,azure_openai_api_model\n",
    "        ,azure_openai_api_temperature\n",
    "        ,azure_openai_api_max_tokens\n",
    "        ,messages\n",
    "        ,filename\n",
    "        )\n",
    "    print(filename)\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert static T-SQL scripts into dynamic T-SQL script files. \n",
    "\n",
    "The required steps are:\n",
    "- Obtain the location of each T-SQL script file.\n",
    "- Create a Python function to read each T-SQL script file.\n",
    "- Use Azure OpenAI to make the script re-runnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name: .\\tsql\\load_staging_data_table.sql\n",
      "TRUNCATE TABLE stg.salestmp;\n",
      "\n",
      "BULK INSERT stg.salestmp\n",
      "FROM 'F:\\Presentation\\t-sql-as-prompts\\csv\\bicycle_data.csv'\n",
      "WITH (\n",
      "    FIRSTROW = 2,\n",
      "    FIELDTERMINATOR = ',',\n",
      "    ROWTERMINATOR = '\\n',\n",
      "    ERRORFILE = 'F:\\Presentation\\t-sql-as-prompts\\csv\\bicycle_data_error.log'\n",
      ");\n",
      "File Name: .\\tsql\\load_prd_data_table.sql\n",
      "INSERT INTO prd.sales (ProductId, ProductName, ProductType, Color, OrderQuantity, Size, Category, Country, Date, PurchasePrice, SellingPrice)\n",
      "SELECT \n",
      "    CAST(ProductId AS INT),\n",
      "    ProductName,\n",
      "    ProductType,\n",
      "    Color,\n",
      "    CAST(OrderQuantity AS INT),\n",
      "    Size,\n",
      "    Category,\n",
      "    Country,\n",
      "    CAST(Date AS DATE),\n",
      "    CAST(PurchasePrice AS DECIMAL(18,2)),\n",
      "    CAST(SellingPrice AS DECIMAL(18,2))\n",
      "FROM stg.salestmp;\n"
     ]
    }
   ],
   "source": [
    "# Create a Python function to read the T-SQL file\n",
    "def read_tsql_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        content = f.read()\n",
    "    return content\n",
    "\n",
    "# Define the T-SQL development scripts file names.\n",
    "keys = {\n",
    "\n",
    "    1: (\"load_staging_data_table_reusable\", \".\\\\tsql\\\\load_staging_data_table.sql\"),\n",
    "    2: (\"load_prd_data_table_reusable\", \".\\\\tsql\\\\load_prd_data_table.sql\"),\n",
    "}\n",
    "\n",
    "for each_tsql_file in keys:\n",
    "    filename= (keys[each_tsql_file][1])\n",
    "    print(\"File Name: \" + filename)    \n",
    "    bulk_sql_script = read_tsql_file(filename)\n",
    "    print(bulk_sql_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\\\Presentation\\\\t-sql-as-prompts\\\\tsql\\\\load_staging_data_table.sql\n",
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      "CREATE PROCEDURE etl_process.usp_BulkInsertFromCSV\n",
      "    @tableName NVARCHAR(255),\n",
      "    @filePath NVARCHAR(255),\n",
      "    @errorFilePath NVARCHAR(255)\n",
      "AS\n",
      "BEGIN\n",
      "    DECLARE @startTime DATETIME;\n",
      "    DECLARE @endTime DATETIME;\n",
      "    DECLARE @errorMsg NVARCHAR(MAX);\n",
      "\n",
      "    BEGIN TRY\n",
      "        SET @startTime = GETDATE();\n",
      "\n",
      "        DECLARE @bulkInsertQuery NVARCHAR(MAX);\n",
      "        SET @bulkInsertQuery = 'TRUNCATE TABLE ' + @tableName + '; ' +\n",
      "            'BULK INSERT ' + @tableName + ' ' +\n",
      "            'FROM ''' + @filePath + ''' ' +\n",
      "            'WITH (FIRSTROW = 2, FIELDTERMINATOR = '','', ROWTERMINATOR = ''\\n'', ERRORFILE = ''' + @errorFilePath + ''');';\n",
      "\n",
      "        EXEC sp_executesql @bulkInsertQuery;\n",
      "\n",
      "        SET @endTime = GETDATE();\n",
      "\n",
      "        EXEC etl_process.usp_get_process_log 'Bulk Insert', 'T-SQL', @tableName, @startTime, @endTime;\n",
      "    END TRY\n",
      "    BEGIN CATCH\n",
      "        SET @endTime = GETDATE();\n",
      "        SET @errorMsg = ERROR_MESSAGE();\n",
      "\n",
      "        EXEC etl_process.usp_get_error_log 'Bulk Insert', @tableName, @errorMsg, @startTime, @endTime;\n",
      "    END CATCH;\n",
      "END;\n",
      "F:\\\\Presentation\\\\t-sql-as-prompts\\\\tsql\\\\load_prd_data_table.sql\n",
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      "CREATE PROCEDURE etl_process.usp_InsertSalesData\n",
      "AS\n",
      "BEGIN\n",
      "    SET NOCOUNT ON;\n",
      "    \n",
      "    DECLARE @processname VARCHAR(100) = 'Insert Sales Data';\n",
      "    DECLARE @processtype VARCHAR(100) = 'ETL';\n",
      "    DECLARE @objectname VARCHAR(100) = 'prd.sales';\n",
      "    DECLARE @starttime DATETIME = GETDATE();\n",
      "    DECLARE @endtime DATETIME;\n",
      "    DECLARE @errormsg VARCHAR(MAX);\n",
      "    \n",
      "    BEGIN TRY\n",
      "        -- Insert data into prd.sales table\n",
      "        INSERT INTO prd.sales (ProductId, ProductName, ProductType, Color, OrderQuantity, Size, Category, Country, Date, PurchasePrice, SellingPrice)\n",
      "        SELECT \n",
      "            CAST(ProductId AS INT),\n",
      "            ProductName,\n",
      "            ProductType,\n",
      "            Color,\n",
      "            CAST(OrderQuantity AS INT),\n",
      "            Size,\n",
      "            Category,\n",
      "            Country,\n",
      "            CAST(Date AS DATE),\n",
      "            CAST(PurchasePrice AS DECIMAL(18,2)),\n",
      "            CAST(SellingPrice AS DECIMAL(18,2))\n",
      "        FROM stg.salestmp;\n",
      "        \n",
      "        SET @endtime = GETDATE();\n",
      "        \n",
      "        -- Log the process details\n",
      "        EXEC etl_process.usp_get_process_log @processname, @processtype, @objectname, @starttime, @endtime;\n",
      "    END TRY\n",
      "    BEGIN CATCH\n",
      "        SET @endtime = GETDATE();\n",
      "        SET @errormsg = ERROR_MESSAGE();\n",
      "        \n",
      "        -- Log the error details\n",
      "        EXEC etl_process.usp_get_error_log @processname, @objectname, @errormsg, @starttime, @endtime;\n",
      "    END CATCH;\n",
      "END;\n",
      "GO\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# from pprint import pprint\n",
    "\n",
    "json_file = 'bicycle_data_prompt.json'\n",
    "\n",
    "with open(json_file) as json_data:\n",
    "    data = json.load(json_data)\n",
    "\n",
    "\n",
    "# Define the T-SQL development scripts file names.\n",
    "keys = {\n",
    "\n",
    "    1: (\"load_staging_data_table_reusable\", \"load_staging_data_table.sql\"),\n",
    "    2: (\"load_prd_data_table_reusable\", \"load_prd_data_table.sql\"),\n",
    "}\n",
    "\n",
    "deploymenttype = \"MakeDatabaseObjectReusable\"\n",
    "\n",
    "system_role = data[deploymenttype][0]['system_role']\n",
    "filedir = r\"F:\\\\Presentation\\\\t-sql-as-prompts\\\\tsql\\\\\"\n",
    "index = 1\n",
    "while index <= len(keys):\n",
    "    #print(keys[index])\n",
    "    get_user_input_section = (keys[index][0])\n",
    "    get_user_input = (data[deploymenttype][index][get_user_input_section])\n",
    "    tsql_file_path = filedir+(keys[index][1])\n",
    "    # Read the T-SQL file\n",
    "    get_tsql_script_from_file = read_tsql_file(tsql_file_path)\n",
    "    user_prompt = get_user_input + '\\n'+ get_tsql_script_from_file\n",
    "    # Define the messages to be sent to the Azure OpenAI API\n",
    "    messages = [{\"role\": \"system\", \"content\": system_role},{\"role\": \"user\", \"content\": user_prompt}]\n",
    "    if __name__ == '__main__': \n",
    "        print(tsql_file_path)\n",
    "        SendRequestToAzureOpenAI(client,azure_openai_api_model,azure_openai_api_temperature,azure_openai_api_max_tokens,messages,tsql_file_path)\n",
    "    #print(user_prompt)\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data Into SQL Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Python function to access SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "\n",
    "def execute_sql_script(sql_script):\n",
    "    # Create a connection string\n",
    "    conn_str = (\n",
    "        r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "        r\"SERVER={az-vm-esi-labs};\"\n",
    "        r'DATABASE=master;'\n",
    "        r'Trusted_Connection=yes;'\n",
    "    )\n",
    "    # Establish a connection with the database\n",
    "    conn = pyodbc.connect(conn_str)\n",
    "\n",
    "    # Create a cursor from the connection\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Execute the SQL script\n",
    "    cursor.execute(sql_script)\n",
    "    # Fetch the results\n",
    "    results = cursor.fetchall()\n",
    "    # Commit the transaction if no errors\n",
    "    conn.commit()\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying if SQL Server is accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('master',),\n",
       " ('tempdb',),\n",
       " ('model',),\n",
       " ('msdb',),\n",
       " ('az_synapse_link',),\n",
       " ('dp300_demo_db',),\n",
       " ('AdventureWorksLT2019',),\n",
       " ('AdventureWorks2019',),\n",
       " ('AdventureWorksDW2022',),\n",
       " ('AdventureWorks2022',),\n",
       " ('AdventureWorksLT2022',),\n",
       " ('AdventureWorksDW2020',),\n",
       " ('TailspinToys2020-US',),\n",
       " ('azureopenai',),\n",
       " ('SkillUpAI',)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_script = \"SELECT name FROM sys.databases\"\n",
    "sql_server = \"az-vm-esi-labs\"\n",
    "execute_sql_script(sql_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To save time, let’s create some objects and load the data to finish the manual process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql_script_file(server, database, script_file):\n",
    "    # Establish a connection to the database\n",
    "    conn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER=' + server + ';DATABASE=' + database + ';Trusted_Connection=yes;')\n",
    "\n",
    "    # Create a cursor from the connection\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Read and execute the script file line by line\n",
    "    if script_file.endswith(\".sql\"):\n",
    "        with open(script_file, \"r\") as sql_file:\n",
    "            sql_query = \"\"\n",
    "            for line in sql_file:\n",
    "                # If the line is a GO statement, execute the query and reset it\n",
    "                if line.strip() == \"GO\":\n",
    "                    cursor.execute(sql_query)\n",
    "                    sql_query = \"\"\n",
    "                # Otherwise, append the line to the query\n",
    "                else:\n",
    "                    sql_query += line\n",
    "            # Execute any remaining query\n",
    "            if sql_query:\n",
    "                cursor.execute(sql_query)\n",
    "    else:\n",
    "        sql_query = script_file\n",
    "        cursor.execute(sql_query)\n",
    "    # Commit the transaction\n",
    "    conn.commit()\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize T-SQL development scripts file names.\n",
    "keys = {\n",
    "    1: (\"createprocessschema\", \".\\\\tsql\\\\create_etl_process_schema.sql\"),\n",
    "    2: (\"createstageschema\", \".\\\\tsql\\\\create_stage_schema.sql\"),\n",
    "    3: (\"createsprdchema\", \".\\\\tsql\\\\creates_prd_chema.sql\"),\n",
    "    4: (\"createprocessschema\", \".\\\\tsql\\\\create_etl_process_schema.sql\"),\n",
    "    5: (\"createprocesslogtable\", \".\\\\tsql\\\\create_etl_processlog_table.sql\"),\n",
    "    6: (\"createbatcherrorlogtable\", \".\\\\tsql\\\\create_etl_errorlog_table.sql\"),\n",
    "    7: (\"createprocesslogsp\", \".\\\\tsql\\\\create_etl_processlog_usp.sql\"),\n",
    "    8: (\"createerrorlogsp\", \".\\\\tsql\\\\create_etl_errorlog_usp.sql\"),\n",
    "    9: (\"createstagetable\", \".\\\\tsql\\\\create_stage_table.sql\"),\n",
    "    10: (\"createprdtable\", \".\\\\tsql\\\\create_prd_table.sql\"),\n",
    "    11: (\"loadstagingdatatable\", \".\\\\tsql\\\\load_staging_data_table.sql\"),\n",
    "    12: (\"loadprddatatable\", \".\\\\tsql\\\\load_prd_data_table.sql\"),\n",
    "    13: (\"createview\", \".\\\\tsql\\\\create_view.sql\"),\n",
    "    14: (\"createprocedure\", \".\\\\tsql\\\\create_procedure.sql\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\tsql\\create_etl_process_schema.sql\n",
      ".\\tsql\\create_stage_schema.sql\n",
      ".\\tsql\\creates_prd_chema.sql\n",
      ".\\tsql\\create_etl_process_schema.sql\n",
      ".\\tsql\\create_etl_processlog_table.sql\n",
      ".\\tsql\\create_etl_errorlog_table.sql\n",
      ".\\tsql\\create_etl_processlog_usp.sql\n",
      ".\\tsql\\create_etl_errorlog_usp.sql\n",
      ".\\tsql\\create_stage_table.sql\n",
      ".\\tsql\\create_prd_table.sql\n",
      ".\\tsql\\load_staging_data_table.sql\n",
      ".\\tsql\\load_prd_data_table.sql\n",
      ".\\tsql\\create_view.sql\n",
      ".\\tsql\\create_procedure.sql\n"
     ]
    }
   ],
   "source": [
    "database='SkillUpAI'\n",
    "server='az-vm-esi-labs'\n",
    "index = 1\n",
    "while index<= len(keys):\n",
    "    print(keys[index][1])\n",
    "    script_file = keys[index][1]\n",
    "    execute_sql_script_file(sql_server, database, script_file)\n",
    "    index+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Let us perform the initial CSV bulk load.**\n",
    "- It will import the CSV file data into SQL Server stage table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store procedure to load data into stage table\n",
    "script_file=\"\"\"\n",
    "EXECUTE etl_process.usp_BulkInsertFromCSV \n",
    "\t\t\t 'stg.salestmp'\n",
    "\t\t\t,'F:\\\\Presentation\\\\t-sql-as-prompts\\\\csv\\\\bicycle_data.csv'\n",
    "\t\t\t,'F:\\\\Presentation\\\\t-sql-as-prompts\\\\csv\\\\errorload.csv'\n",
    "\"\"\"\n",
    "database='skillupai'\n",
    "server='az-vm-esi-labs'\n",
    "execute_sql_script_file(server, database, script_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load Bicycle Sales data from stage to destination**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store procedure to load data into stage table\n",
    "script_file=\"\"\"\n",
    "EXECUTE etl_process.usp_InsertSalesData\n",
    "\"\"\"\n",
    "database='skillupai'\n",
    "server='az-vm-esi-labs'\n",
    "execute_sql_script_file(server, database, script_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iiii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store procedure to load data into stage table\n",
    "script_file=\"\"\"\n",
    "EXECUTE etl_process.usp_BulkInsertFromCSV \n",
    "\t\t\t 'stg.salestmp'\n",
    "\t\t\t,'F:\\\\Presentation\\\\t-sql-as-prompts\\\\csv\\\\bicycle_data_bad.csv'\n",
    "\t\t\t,'F:\\\\Presentation\\\\t-sql-as-prompts\\\\csv\\\\errorload.csv'\n",
    "\"\"\"\n",
    "database='skillupai'\n",
    "server='az-vm-esi-labs'\n",
    "execute_sql_script_file(server, database, script_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#T-SQL Query to check for the bad data\n",
    "SELECT *  FROM [skillupai].[stg].[salestmp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Demo TSQL Script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql_script\n",
    "CREATE DATABASE skillupai\n",
    "GO\n",
    "\n",
    "-- Copy and past to ssms\n",
    "SELECT TOP (5) * FROM [skillupai].[stg].[salestmp];\n",
    "GO\n",
    "\n",
    "SELECT TOP (5) * FROM [skillupai].[prd].[sales];\n",
    "GO\n",
    "\n",
    "SELECT TOP (5) * FROM [skillupai].[etl_process].[etl_process_log];\n",
    "GO\n",
    "\n",
    "SELECT TOP (5) * FROM [skillupai].[etl_process].[error_log];\n",
    "GO\n",
    "\n",
    "-- View\n",
    "SELECT TOP (5) * FROM [skillupai].[prd].[vw_GetSaleDetails];\n",
    "GO\n",
    "\n",
    "-- Proc\n",
    "EXECUTE [prd].[usp_GetTotalSalesByCountries]\n",
    "GO\n",
    "\n",
    "-- Load bad csv file\n",
    "--EXECUTE etl_process.usp_InsertSalesData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Things to Consider**\n",
    "When developing a database using prompts as T-SQL development, there are some things to consider, such as:\n",
    "- **Prompt quality:** The quality of the prompt affects the quality of the generated code. A good prompt should be clear, concise, and specific, and avoid ambiguity, vagueness, and redundancy. A good prompt should also provide enough context and information for the generative AI model to understand the intent and the desired output of the prompt.\n",
    "Code quality: The quality of the generated code depends on the quality of the generative AI model and the prompt. A good code should be correct, complete, and consistent, and follow the best practices and standards of T-SQL coding. A good code should also be readable, maintainable, and well-documented, and avoid errors, bugs, and vulnerabilities.\n",
    "\n",
    "- **Code validation:** The generated code should be validated before executing or deploying it to the database. We can use various tools and methods to validate the code, such as syntax checking, code formatting, code analysis, code review, or code testing. We can also compare the generated code with the expected code or the existing code to ensure its accuracy and compatibility.\n",
    "\n",
    "- **Code modification:** The generated code may not always meet our expectations or requirements, and may need some modification or improvement. We can use various tools and techniques to modify the code, such as code editing, code refactoring, code debugging, or code optimization. We can also use feedback loops or interactive sessions to refine the prompts and the generated code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Microsoft Autogen as a Tool**\n",
    "Microsoft Autogen is a tool that simplifies the orchestration, automation, and optimization of LLM workflows. It offers customizable and conversable agents that leverage the strongest capabilities of the most advanced LLMs, such as GPT-4, while addressing their limitations by integrating with humans and tools and having conversations between multiple agents via automated chat.\n",
    "\n",
    "Microsoft Autogen can be a useful tool for database development using prompts as T-SQL development, as it can enable us to:\n",
    "- **Build complex multi-agent conversation systems:** We can use Microsoft Autogen to define a set of agents with specialized capabilities and roles, such as Commander, Writer, Safeguard, and Executor, and define the interaction behavior between agents, such as what to reply when an agent receives messages from another agent. This way, we can build complex systems that can handle various tasks, such as code-based question answering, supply-chain optimization, conversational chess, and more.\n",
    "\n",
    "- **Simplify the orchestration, automation, and optimization of LLM workflows:** We can use Microsoft Autogen to simplify the orchestration, automation, and optimization of LLM workflows, such as error handling, multi-config inference, context programming, and more. This way, we can reduce the coding effort, improve the code quality, and enhance the LLM inference.\n",
    "\n",
    "- **Support diverse conversation patterns for complex workflows:** We can use Microsoft Autogen to support diverse conversation patterns for complex workflows, such as conversation autonomy, the number of agents, and agent conversation topology. This way, we can customize and compose the agents and their interactions to suit our needs and preferences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
